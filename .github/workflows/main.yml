name: Daily News Fetch and Dataset Update

on:
  schedule:
    # Run at 8:00 AM, 4:00 PM, and 11:00 PM UTC
    - cron: '0 8,16,23 * * *'
  workflow_dispatch: # Allow manual triggering

env:
  PYTHON_VERSION: '3.11'
  
jobs:
  fetch-and-update-dataset:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        # Install additional packages for dataset handling
        pip install datasets huggingface_hub pandas

    - name: Configure git
      run: |
        git config --global user.name "GitHub Actions Bot"
        git config --global user.email "actions@github.com"

    - name: Clone dataset repository
      env:
        HF_TOKEN: ${{ secrets.HF_TOKEN }}
      run: |
        # Clone the private dataset repository with authentication
        git clone https://oauth2:$HF_TOKEN@huggingface.co/datasets/marianbasti/news-ar dataset_repo

    - name: Fetch and append news articles
      run: |
        python -c "
        import sys
        import os
        import json
        import pandas as pd
        from datetime import datetime, timezone
        sys.path.append('.')
        from app.services.rss_fetcher import fetch_all_articles
        import logging

        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger(__name__)

        articles_path = 'dataset_repo/articles.jsonl'
        new_articles_path = 'new_articles/articles.jsonl'

        # Load existing articles
        existing_articles = []
        if os.path.exists(articles_path):
            with open(articles_path, 'r', encoding='utf-8') as f:
                for line in f:
                    try:
                        existing_articles.append(json.loads(line))
                    except Exception:
                        continue

        # Fetch new articles
        try:
            logger.info('Fetching articles from RSS feeds...')
            articles = fetch_all_articles()
            logger.info(f'Fetched {len(articles)} articles')
            if articles:
                articles_data = []
                for article in articles:
                    article_data = {
                        'title': article.title,
                        'url': str(article.url),
                        'source_name': article.source_name,
                        'source_type': article.source_type,
                        'content': article.content or '',
                        'summary': article.summary or '',
                        'fetched_date': article.fetched_date.isoformat() if article.fetched_date else datetime.now(timezone.utc).isoformat(),
                        'publication_date': article.publication_date.isoformat() if article.publication_date else None,
                        'export_timestamp': datetime.now(timezone.utc).isoformat()
                    }
                    articles_data.append(article_data)

                # Merge and deduplicate by URL
                all_articles = {a['url']: a for a in existing_articles}
                for a in articles_data:
                    all_articles[a['url']] = a

                merged_articles = list(all_articles.values())
                logger.info(f'Merged dataset with {len(merged_articles)} articles')

                os.makedirs('new_articles', exist_ok=True)
                # Save as JSONL
                with open(new_articles_path, 'w', encoding='utf-8') as f:
                    for article in merged_articles:
                        f.write(json.dumps(article, ensure_ascii=False) + '\n')
                # Save as CSV
                df = pd.DataFrame(merged_articles)
                df.to_csv('new_articles/articles.csv', index=False, encoding='utf-8')
                # Save as Parquet
                df.to_parquet('new_articles/articles.parquet', index=False)
                # Metadata
                metadata = {
                    'export_date': datetime.now(timezone.utc).isoformat(),
                    'total_articles': len(merged_articles),
                    'sources': list(set(a['source_name'] for a in merged_articles if a['source_name'])),
                    'fetch_run_timestamp': datetime.now(timezone.utc).isoformat()
                }
                with open('new_articles/metadata.json', 'w', encoding='utf-8') as f:
                    json.dump(metadata, f, indent=2, ensure_ascii=False)
                logger.info('Articles exported and appended successfully')
            else:
                logger.warning('No new articles found')
        except Exception as e:
            logger.error(f'Error in article fetching: {e}', exc_info=True)
            sys.exit(1)
        "
        
    - name: Commit and push dataset changes
      env:
        HF_TOKEN: ${{ secrets.HF_TOKEN }}
      run: |
        # Copy new articles to dataset repository
        if [ -d "new_articles" ]; then
          echo "Copying files to dataset repository..."
          cp new_articles/* dataset_repo/ 2>/dev/null || true
          ls -la new_articles/
          echo "Files in dataset_repo after copy:"
          ls -la dataset_repo/
        fi
        
        cd dataset_repo
        
        # Check if there are changes
        if [ -n "$(git status --porcelain)" ]; then
          echo "Changes detected, committing..."
          git status
          
          # Add all files
          git add .
          
          # Get article count from the correct file
          ARTICLE_COUNT=$(wc -l < articles.jsonl 2>/dev/null || echo "0")
          
          # Create commit message
          git commit -m "Daily update: $(date -u '+%Y-%m-%d %H:%M UTC') - $ARTICLE_COUNT articles"
          
          # Push changes
          git push
          
          echo "✅ Dataset updated successfully with $ARTICLE_COUNT articles"
        else
          echo "ℹ️ No changes to commit"
          echo "Current files in dataset repo:"
          ls -la
        fi

    - name: Cleanup
      if: always()
      run: |
        # Clean up temporary files
        rm -rf new_articles dataset_repo
