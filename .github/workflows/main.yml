name: Daily News Fetch and Dataset Update

on:
  schedule:
    # Run at 8:00 AM, 4:00 PM, and 11:00 PM UTC
    - cron: '0 8,16,23 * * *'
  workflow_dispatch: # Allow manual triggering

env:
  PYTHON_VERSION: '3.11'
  
jobs:
  fetch-and-update-dataset:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        # Install additional packages for dataset handling
        pip install datasets huggingface_hub pandas

    - name: Configure git
      run: |
        git config --global user.name "GitHub Actions Bot"
        git config --global user.email "actions@github.com"

    - name: Clone dataset repository
      env:
        HF_TOKEN: ${{ secrets.HF_TOKEN }}
      run: |
        # Clone the private dataset repository with authentication
        git clone https://oauth2:$HF_TOKEN@huggingface.co/datasets/marianbasti/news-ar dataset_repo

    - name: Fetch news articles
      run: |
        python -c "
        import sys
        import os
        import json
        import pandas as pd
        from datetime import datetime, timezone
        sys.path.append('.')
        
        from app.services.rss_fetcher import fetch_all_articles
        import logging
        
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger(__name__)
        
        try:
            logger.info('Fetching articles from RSS feeds...')
            articles = fetch_all_articles()
            logger.info(f'Fetched {len(articles)} articles')
            
            if articles:
                # Convert articles to dict format
                articles_data = []
                for article in articles:
                    article_data = {
                        'title': article.title,
                        'url': str(article.url),
                        'source_name': article.source_name,
                        'source_type': article.source_type,
                        'content': article.content or '',
                        'summary': article.summary or '',
                        'fetched_date': article.fetched_date.isoformat() if article.fetched_date else datetime.now(timezone.utc).isoformat(),
                        'publication_date': article.publication_date.isoformat() if article.publication_date else None,
                        'export_timestamp': datetime.now(timezone.utc).isoformat()
                    }
                    articles_data.append(article_data)
                
                logger.info(f'Preparing dataset with {len(articles_data)} articles')
                
                # Create dataset directory
                os.makedirs('new_articles', exist_ok=True)
                
                # Save as JSON Lines (most versatile)
                with open('new_articles/articles.jsonl', 'w', encoding='utf-8') as f:
                    for article in articles_data:
                        f.write(json.dumps(article, ensure_ascii=False) + '\n')
                
                # Save as CSV
                df = pd.DataFrame(articles_data)
                df.to_csv('new_articles/articles.csv', index=False, encoding='utf-8')
                
                # Save as Parquet (most efficient)
                df.to_parquet('new_articles/articles.parquet', index=False)
                
                # Create metadata file
                metadata = {
                    'export_date': datetime.now(timezone.utc).isoformat(),
                    'total_articles': len(articles_data),
                    'sources': list(set(a['source_name'] for a in articles_data if a['source_name'])),
                    'fetch_run_timestamp': datetime.now(timezone.utc).isoformat()
                }
                
                with open('new_articles/metadata.json', 'w', encoding='utf-8') as f:
                    json.dump(metadata, f, indent=2, ensure_ascii=False)
                
                logger.info(f'Articles exported successfully: {len(articles_data)} articles')
                logger.info(f'Sources: {len(metadata[\"sources\"])}')
                
            else:
                logger.warning('No articles found')
                # Create empty files to avoid issues
                os.makedirs('new_articles', exist_ok=True)
                with open('new_articles/articles.jsonl', 'w') as f:
                    pass
                
        except Exception as e:
            logger.error(f'Error in article fetching: {e}', exc_info=True)
            sys.exit(1)
        "

    - name: Commit and push dataset changes
      env:
        HF_TOKEN: ${{ secrets.HF_TOKEN }}
      run: |
        # Copy new articles to dataset repository
        if [ -d "new_articles" ]; then
          echo "Copying files to dataset repository..."
          cp new_articles/* dataset_repo/ 2>/dev/null || true
          ls -la new_articles/
          echo "Files in dataset_repo after copy:"
          ls -la dataset_repo/
        fi
        
        cd dataset_repo
        
        # Check if there are changes
        if [ -n "$(git status --porcelain)" ]; then
          echo "Changes detected, committing..."
          git status
          
          # Add all files
          git add .
          
          # Get article count from the correct file
          ARTICLE_COUNT=$(wc -l < articles.jsonl 2>/dev/null || echo "0")
          
          # Create commit message
          git commit -m "Daily update: $(date -u '+%Y-%m-%d %H:%M UTC') - $ARTICLE_COUNT articles"
          
          # Push changes
          git push
          
          echo "✅ Dataset updated successfully with $ARTICLE_COUNT articles"
        else
          echo "ℹ️ No changes to commit"
          echo "Current files in dataset repo:"
          ls -la
        fi

    - name: Cleanup
      if: always()
      run: |
        # Clean up temporary files
        rm -rf new_articles dataset_repo
