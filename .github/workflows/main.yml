name: Daily News Fetch and Dataset Update

on:
  schedule:
    # Run at 8:00 AM, 4:00 PM, and 11:00 PM UTC
    - cron: '0 8,16,23 * * *'
  workflow_dispatch: # Allow manual triggering

env:
  PYTHON_VERSION: '3.11'
  
jobs:
  fetch-and-update-dataset:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        # Install additional packages for dataset handling
        pip install datasets huggingface_hub pandas

    - name: Install Hugging Face CLI
      run: |
        pip install huggingface_hub[cli]

    - name: Fetch and export news articles
      run: |
        python -c "
        import sys
        import os
        import json
        import pandas as pd
        from datetime import datetime, timezone
        sys.path.append('.')
        from app.services.rss_fetcher import fetch_all_articles
        import logging

        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger(__name__)

        new_articles_path = 'new_articles/articles.jsonl'

        # Fetch new articles
        try:
            logger.info('Fetching articles from RSS feeds...')
            articles = fetch_all_articles()
            logger.info(f'Fetched {len(articles)} articles')
            if articles:
                articles_data = []
                for article in articles:
                    article_data = {
                        'title': article.title,
                        'url': str(article.url),
                        'source_name': article.source_name,
                        'source_type': article.source_type,
                        'content': article.content or '',
                        'summary': article.summary or '',
                        'fetched_date': article.fetched_date.isoformat() if article.fetched_date else datetime.now(timezone.utc).isoformat(),
                        'publication_date': article.publication_date.isoformat() if article.publication_date else None,
                        'export_timestamp': datetime.now(timezone.utc).isoformat()
                    }
                    articles_data.append(article_data)

                logger.info(f'Prepared {len(articles_data)} articles for export')

                os.makedirs('new_articles', exist_ok=True)
                # Save as JSONL
                with open(new_articles_path, 'w', encoding='utf-8') as f:
                    for article in articles_data:
                        f.write(json.dumps(article, ensure_ascii=False) + '\n')
                # Save as CSV
                df = pd.DataFrame(articles_data)
                df.to_csv('new_articles/articles.csv', index=False, encoding='utf-8')
                # Save as Parquet
                df.to_parquet('new_articles/articles.parquet', index=False)
                # Metadata
                metadata = {
                    'export_date': datetime.now(timezone.utc).isoformat(),
                    'total_articles': len(articles_data),
                    'sources': list(set(a['source_name'] for a in articles_data if a['source_name'])),
                    'fetch_run_timestamp': datetime.now(timezone.utc).isoformat()
                }
                with open('new_articles/metadata.json', 'w', encoding='utf-8') as f:
                    json.dump(metadata, f, indent=2, ensure_ascii=False)
                logger.info('Articles exported successfully')
            else:
                logger.warning('No new articles found')
        except Exception as e:
            logger.error(f'Error in article fetching: {e}', exc_info=True)
            sys.exit(1)
        "
        
    - name: Upload to Hugging Face Hub
      env:
        HF_TOKEN: ${{ secrets.HF_TOKEN }}
      run: |
        # Set up authentication with Hugging Face
        huggingface-cli login --token $HF_TOKEN
        
        # Check if new_articles directory exists and upload files
        if [ -d "new_articles" ]; then
          echo "Uploading files to Hugging Face Hub..."
          ls -la new_articles/
          
          # Upload each file if it exists
          for file in articles.jsonl articles.csv articles.parquet metadata.json; do
            if [ -f "new_articles/$file" ]; then
              echo "Uploading $file..."
              huggingface-cli upload marianbasti/news-ar "new_articles/$file" "$file"
            else
              echo "File $file not found, skipping..."
            fi
          done
          
          # Get article count for the status message
          ARTICLE_COUNT=$(wc -l < new_articles/articles.jsonl 2>/dev/null || echo "0")
          echo "✅ Dataset updated successfully with $ARTICLE_COUNT articles"
        else
          echo "ℹ️ No new_articles directory found, nothing to upload"
        fi

    - name: Cleanup
      if: always()
      run: |
        # Clean up temporary files
        rm -rf new_articles
